{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fbe7cea-b614-43a7-a8cc-24b09ed69882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# from openslide import OpenSlide\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import (\n",
    "    ConcatDataset,\n",
    "    DataLoader,\n",
    "    Dataset,\n",
    "    Subset,\n",
    "    SubsetRandomSampler,\n",
    "    TensorDataset,\n",
    "    random_split,\n",
    ")\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# import einops\n",
    "\n",
    "# from eval_metrics import print_metrics_regression\n",
    "from sklearn import metrics as sklearn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e71cd1-5d16-472f-80be-28d5fba31e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"./datasets/train.pkl\")\n",
    "train_x = train[\"x\"]\n",
    "train_y = train[\"y\"]\n",
    "train_id = train[\"id\"]\n",
    "train_x = torch.tensor(torch.stack(train_x).detach().cpu().numpy())\n",
    "train_y = torch.tensor(train_y)\n",
    "\n",
    "test = pd.read_pickle(\"./datasets/test.pkl\")\n",
    "test_x = test[\"x\"]\n",
    "test_y = test[\"y\"]\n",
    "test_id = test[\"id\"]\n",
    "test_x = torch.tensor(torch.stack(test_x).detach().cpu().numpy())\n",
    "test_y = torch.tensor(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd251bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 4.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_label = train_y.min().item()\n",
    "max_label = train_y.max().item()\n",
    "train_y = (train_y-min_label)/(max_label-min_label)\n",
    "test_y = (test_y-min_label)/(max_label-min_label)\n",
    "\n",
    "min_label, max_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81058b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_norm(x, min_label=min_label, max_label=max_label):\n",
    "    return (x-min_label)/(max_label-min_label)\n",
    "\n",
    "def reverse_min_max_norm(x, min_label=min_label, max_label=max_label):\n",
    "    return x*(max_label-min_label)+min_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "348c3427-30e6-4d2e-a3b0-93beda382eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10206, 3, 224, 224]),\n",
       " torch.Size([10206]),\n",
       " 10206,\n",
       " torch.Size([10205, 3, 224, 224]),\n",
       " torch.Size([10205]),\n",
       " 10205)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, len(train_id), test_x.shape, test_y.shape, len(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf8426b-56f3-4413-91ff-632597396188",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, x, y, biopsy_id):\n",
    "        self.x = x # img_tensor_list\n",
    "        self.y = y # label\n",
    "        self.biopsy_id = biopsy_id\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index], self.biopsy_id[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7aec716-267c-45e4-a93d-ec074edd8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "epochs = 50\n",
    "learning_rate = 2e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 0 # 1e-8\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d59a4783-0f99-4928-a666-f5a723c37bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_x, train_y, train_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataset = ImageDataset(test_x, test_y, test_id)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b24ec54a-957e-424d-9ca7-1e601afa333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in train_dataset:\n",
    "#     x, y, biopsy_id = data\n",
    "#     print(x.shape, y, biopsy_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1051102c-ceb1-4271-8444-7eaeba3bdd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    return loss_fn(y_pred, y_true)\n",
    "\n",
    "def focal_mse_loss(inputs, targets, activate='sigmoid', beta=.2, gamma=1):\n",
    "    loss = (inputs - targets) ** 2\n",
    "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
    "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def huber_loss(inputs, targets, beta=1.):\n",
    "    l1_loss = torch.abs(inputs - targets)\n",
    "    cond = l1_loss < beta\n",
    "    loss = torch.where(cond, 0.5 * l1_loss ** 2 / beta, l1_loss - 0.5 * beta)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "criterion = mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0b0f76-e41e-440b-93e8-9a605e5799b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, scheduler):\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "    for step, data in enumerate(dataloader):\n",
    "        batch_x, batch_y, batch_biopsy_id = data\n",
    "        batch_x, batch_y = (\n",
    "            batch_x.float().to(device),\n",
    "            batch_y.float().to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        # print(batch_x.device, batch_x.shape)\n",
    "        # print(next(model.parameters()).is_cuda)\n",
    "        output = model(batch_x)\n",
    "        output = torch.squeeze(output, dim=1)\n",
    "        # print(output.shape, batch_y.shape)\n",
    "        \n",
    "        loss = loss_fn(output, batch_y)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    metric_train_loss = np.array(train_loss).mean()\n",
    "    scheduler.step(metric_train_loss)\n",
    "    return metric_train_loss\n",
    "\n",
    "def val_epoch(model, dataloader):\n",
    "    y_pred = {} # key: biopsy_id, value: List[slice_stage_pred]\n",
    "    y_true = {} # key: biopsy_id, value: List[slice_stage_pred]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(dataloader):\n",
    "            # print(step)\n",
    "            batch_x, batch_y, batch_biopsy_id = data\n",
    "            batch_x, batch_y = (\n",
    "                batch_x.float().to(device),\n",
    "                batch_y.float().to(device),\n",
    "            )\n",
    "            output = model(batch_x)\n",
    "            output = torch.squeeze(output, dim=1)\n",
    "            output = output.detach().cpu().numpy().tolist()\n",
    "            batch_y = batch_y.detach().cpu().numpy().tolist()\n",
    "\n",
    "            for i in range(len(batch_biopsy_id)):\n",
    "                biopsy_id = batch_biopsy_id[i]\n",
    "                if biopsy_id not in y_pred:\n",
    "                    y_pred[biopsy_id] = []\n",
    "                    y_true[biopsy_id] = []\n",
    "                y_pred[biopsy_id].append(output[i])\n",
    "                y_true[biopsy_id].append(batch_y[i])\n",
    "    \n",
    "    prediction_list = []\n",
    "    ground_truth_list = []\n",
    "    for biopsy_id in y_pred:\n",
    "        preds = np.array(y_pred[biopsy_id])\n",
    "        truths = np.array(y_true[biopsy_id])\n",
    "        prediction_list.append(preds.mean())\n",
    "        ground_truth_list.append(truths.mean())\n",
    "    prediction_list = np.array(prediction_list)\n",
    "    ground_truth_list = np.array(ground_truth_list)\n",
    "    prediction_list = reverse_min_max_norm(prediction_list)\n",
    "    ground_truth_list = reverse_min_max_norm(ground_truth_list)\n",
    "\n",
    "    mse = sklearn_metrics.mean_squared_error(ground_truth_list, prediction_list)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5842d16-104b-4420-9b8b-2ba22a4772ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=128, bias=True)\n",
       "    (1): GELU(approximate=none)\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = torchvision.models.resnet18(num_classes=1)\n",
    "model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "hidden_dim = model.fc.in_features\n",
    "out_dim = 1\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(hidden_dim, hidden_dim//16),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(hidden_dim//16, out_dim),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# model.load_state_dict(torch.load('./checkpoints/resnet18-f37072fd.pth'), strict=False)\n",
    "model.load_state_dict(torch.load('./checkpoints/resnet50-11ad3fa6.pth'), strict=False)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecda0015-3854-45a3-bdeb-46ccc2a963ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.053634231071919204\n",
      "Val Score: 0.692208026844931\n",
      "Saving best model ...\n",
      "Epoch 1: Loss = 0.03878230648115277\n",
      "Val Score: 0.6546129105283747\n",
      "Saving best model ...\n",
      "Epoch 2: Loss = 0.02708146444056183\n",
      "Val Score: 0.6483528409185323\n",
      "Saving best model ...\n",
      "Epoch 3: Loss = 0.018828779296018183\n",
      "Val Score: 0.6791260418825155\n",
      "Epoch 4: Loss = 0.017115258367266507\n",
      "Val Score: 0.8715568246490456\n",
      "Epoch 5: Loss = 0.01586588758509606\n",
      "Val Score: 0.7695032445996475\n",
      "Epoch 6: Loss = 0.01316613903036341\n",
      "Val Score: 0.7646366392928919\n",
      "Epoch 7: Loss = 0.01137981122592464\n",
      "Val Score: 0.6718654984570094\n",
      "Epoch 8: Loss = 0.010101343021960928\n",
      "Val Score: 0.6938697026242245\n",
      "Epoch 9: Loss = 0.00862907776609063\n",
      "Val Score: 0.7457029309545187\n",
      "Epoch 10: Loss = 0.007600214006379247\n",
      "Val Score: 0.8658422557676317\n",
      "Epoch 11: Loss = 0.006624160031788051\n",
      "Val Score: 0.6813749360389835\n",
      "Epoch 12: Loss = 0.00578711973503232\n",
      "Val Score: 0.7335255319291989\n",
      "Epoch 13: Loss = 0.004475378541974351\n",
      "Val Score: 0.694105713798105\n",
      "Epoch 14: Loss = 0.003467668843222782\n",
      "Val Score: 0.6895086813920261\n",
      "Epoch 15: Loss = 0.002731311778188683\n",
      "Val Score: 0.6198044073807458\n",
      "Saving best model ...\n",
      "Epoch 16: Loss = 0.002411466941703111\n",
      "Val Score: 0.6507603607927129\n",
      "Epoch 17: Loss = 0.002276511740637943\n",
      "Val Score: 0.6540266166015494\n",
      "Epoch 18: Loss = 0.002206667943391949\n",
      "Val Score: 0.6636717086442631\n",
      "Epoch 19: Loss = 0.00174673683650326\n",
      "Val Score: 0.6512144103233507\n",
      "Epoch 20: Loss = 0.0013967820836114698\n",
      "Val Score: 0.6509280176793317\n",
      "Epoch 21: Loss = 0.001119283180742059\n",
      "Val Score: 0.6588788850290203\n",
      "Epoch 22: Loss = 0.0010521093361603561\n",
      "Val Score: 0.6382194625837249\n",
      "Epoch 23: Loss = 0.0012219533964525908\n",
      "Val Score: 0.6375620037099291\n",
      "Epoch 24: Loss = 0.001297120563685894\n",
      "Val Score: 0.6406670313088605\n",
      "Epoch 25: Loss = 0.0012695213052211329\n",
      "Val Score: 0.6444108722206812\n",
      "Epoch 26: Loss = 0.0010458416538313032\n",
      "Val Score: 0.6505916510412737\n",
      "Epoch 27: Loss = 0.0008892926751286722\n",
      "Val Score: 0.615047228315182\n",
      "Saving best model ...\n",
      "Epoch 28: Loss = 0.0009451653226278722\n",
      "Val Score: 0.6666496990717607\n",
      "Epoch 29: Loss = 0.0009468600197578781\n",
      "Val Score: 0.6822312266061245\n",
      "Epoch 30: Loss = 0.0009396780878887512\n",
      "Val Score: 0.6545238636974438\n",
      "Epoch 31: Loss = 0.0008695884091139306\n",
      "Val Score: 0.6453175932897894\n",
      "Epoch 32: Loss = 0.000730968096468132\n",
      "Val Score: 0.651068039156212\n",
      "Epoch 33: Loss = 0.0006027052084391471\n",
      "Val Score: 0.653079371090698\n",
      "Epoch 34: Loss = 0.00048539609051658774\n",
      "Val Score: 0.650260273229672\n",
      "Epoch 35: Loss = 0.0004610187857906567\n",
      "Val Score: 0.64103495094351\n",
      "Epoch 36: Loss = 0.0003869683267112123\n",
      "Val Score: 0.6326015006994151\n",
      "Epoch 37: Loss = 0.00035748810441873504\n",
      "Val Score: 0.6475508989800316\n",
      "Epoch 38: Loss = 0.00031890099598967935\n",
      "Val Score: 0.6417710495159533\n",
      "Epoch 39: Loss = 0.000336437031000969\n",
      "Val Score: 0.6391074025877145\n",
      "Epoch 40: Loss = 0.0003050285289646126\n",
      "Val Score: 0.6533128540173748\n",
      "Epoch 41: Loss = 0.00032678599454811775\n",
      "Val Score: 0.6720067744446271\n",
      "Epoch 42: Loss = 0.0003580072061595274\n",
      "Val Score: 0.6482628656489874\n",
      "Epoch 43: Loss = 0.0003449191102845361\n",
      "Val Score: 0.6492686224887791\n",
      "Epoch 44: Loss = 0.0003050726092624245\n",
      "Val Score: 0.6325267860609768\n",
      "Epoch 45: Loss = 0.00039757504491717556\n",
      "Val Score: 0.6382394796222476\n",
      "Epoch 46: Loss = 0.0004336163361585932\n",
      "Val Score: 0.6591905175282775\n",
      "Epoch 47: Loss = 0.0004233828585711308\n",
      "Val Score: 0.6685407078701698\n",
      "Epoch 48: Loss = 0.0005219198686972959\n",
      "Val Score: 0.6618033962245465\n",
      "Epoch 49: Loss = 0.0006425861800380516\n",
      "Val Score: 0.6230214114529666\n"
     ]
    }
   ],
   "source": [
    "best_score = 1e8\n",
    "for epoch in range(epochs):\n",
    "    # print(f'Running epoch {epoch} ...')\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler\n",
    "    )\n",
    "    print(f\"Epoch {epoch}: Loss = {train_loss}\")\n",
    "    if epoch % 1 == 0:\n",
    "        metric_valid = val_epoch(model, test_loader)\n",
    "        print(\"Val Score:\", metric_valid)\n",
    "        if metric_valid < best_score:\n",
    "            best_score = metric_valid\n",
    "            print(\"Saving best model ...\")\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                f\"./checkpoints/model_resnet50.ckpt\",\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c5ece5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.615047228315182"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "best_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('med')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "03b9317a45a69c475f09ce7bb33975b2f1504feff0a69820c9d0e843e17e056b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
