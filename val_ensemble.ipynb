{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3e834e-a4a6-47ba-919d-2c8fc50cf0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# from openslide import OpenSlide\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import (\n",
    "    ConcatDataset,\n",
    "    DataLoader,\n",
    "    Dataset,\n",
    "    Subset,\n",
    "    SubsetRandomSampler,\n",
    "    TensorDataset,\n",
    "    random_split,\n",
    ")\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn import metrics as sklearn_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d9ea1e-bb32-4cb3-9364-4bdc3a4df2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(\"./datasets/v2/test.pkl\")\n",
    "test_x = test[\"x\"]\n",
    "test_y = test[\"y\"]\n",
    "test_id = test[\"id\"]\n",
    "test_x = torch.tensor(torch.stack(test_x).detach().cpu().numpy())\n",
    "test_y = torch.tensor(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831a00be-e925-4306-bfd1-f5aed0ba9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_label = 0.\n",
    "max_label = 4.\n",
    "\n",
    "test_y = (test_y-min_label)/(max_label-min_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79bf3482-d9f3-4280-8fe4-68653e461f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_norm(x, min_label=min_label, max_label=max_label):\n",
    "    return (x-min_label)/(max_label-min_label)\n",
    "\n",
    "def reverse_min_max_norm(x, min_label=min_label, max_label=max_label):\n",
    "    return x*(max_label-min_label)+min_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df6ca9e-07ae-4fd8-a590-ff8ff3954f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10205, 3, 224, 224]), torch.Size([10205]), 10205)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape, test_y.shape, len(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "551662ba-a37d-41a7-ace2-455af532eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, x, y, biopsy_id):\n",
    "        self.x = x # img_tensor_list\n",
    "        self.y = y # label\n",
    "        self.biopsy_id = biopsy_id\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index], self.biopsy_id[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d605372-9600-462b-9113-4fce53e219fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "epochs = 50\n",
    "learning_rate = 2e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 0 # 1e-8\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6220a54a-109f-47b1-8edc-4883c0127482",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageDataset(test_x, test_y, test_id)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d54ed381-0748-4b40-89ab-e8fce78ada16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    return loss_fn(y_pred, y_true)\n",
    "\n",
    "def focal_mse_loss(inputs, targets, activate='sigmoid', beta=.2, gamma=1):\n",
    "    loss = (inputs - targets) ** 2\n",
    "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
    "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def huber_loss(inputs, targets, beta=1.):\n",
    "    l1_loss = torch.abs(inputs - targets)\n",
    "    cond = l1_loss < beta\n",
    "    loss = torch.where(cond, 0.5 * l1_loss ** 2 / beta, l1_loss - 0.5 * beta)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "criterion = mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04480a1b-901c-409a-9e2b-3db3ccae06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(model, dataloader):\n",
    "    y_pred = {} # key: biopsy_id, value: List[slice_stage_pred]\n",
    "    y_true = {} # key: biopsy_id, value: List[slice_stage_pred]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(dataloader):\n",
    "            # print(step)\n",
    "            batch_x, batch_y, batch_biopsy_id = data\n",
    "            batch_x, batch_y = (\n",
    "                batch_x.float().to(device),\n",
    "                batch_y.float().to(device),\n",
    "            )\n",
    "            output = model(batch_x)\n",
    "            output = torch.squeeze(output, dim=1)\n",
    "            output = output.detach().cpu().numpy().tolist()\n",
    "            batch_y = batch_y.detach().cpu().numpy().tolist()\n",
    "\n",
    "            for i in range(len(batch_biopsy_id)):\n",
    "                biopsy_id = batch_biopsy_id[i]\n",
    "                if biopsy_id not in y_pred:\n",
    "                    y_pred[biopsy_id] = []\n",
    "                    y_true[biopsy_id] = []\n",
    "                y_pred[biopsy_id].append(output[i])\n",
    "                y_true[biopsy_id].append(batch_y[i])\n",
    "    \n",
    "    prediction_list = []\n",
    "    ground_truth_list = []\n",
    "    for biopsy_id in y_pred:\n",
    "        preds = np.array(y_pred[biopsy_id])\n",
    "        truths = np.array(y_true[biopsy_id])\n",
    "        prediction_list.append(preds.mean())\n",
    "        ground_truth_list.append(truths.mean())\n",
    "    prediction_list = np.array(prediction_list)\n",
    "    ground_truth_list = np.array(ground_truth_list)\n",
    "    prediction_list = reverse_min_max_norm(prediction_list)\n",
    "    ground_truth_list = reverse_min_max_norm(ground_truth_list)\n",
    "\n",
    "    mse = sklearn_metrics.mean_squared_error(ground_truth_list, prediction_list)\n",
    "    return y_pred, y_true, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cacc4c01-065f-4788-aa88-786bc6915e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=128, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model = torchvision.models.resnet50()\n",
    "\n",
    "hidden_dim = resnet_model.fc.in_features\n",
    "out_dim = 1\n",
    "\n",
    "resnet_model.fc = nn.Sequential(\n",
    "    nn.Linear(hidden_dim, hidden_dim//16),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(hidden_dim//16, out_dim),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "resnet_model.load_state_dict(torch.load('./checkpoints/model_resnet50.ckpt',map_location={'cuda:1': 'cuda:0'}), strict=True)\n",
    "\n",
    "resnet_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2e211c1-c509-4b34-a1a2-32db04ec65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_01, y_true_01, metric_valid_01 = val_epoch(resnet_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4136ef9-64c8-4933-8e26-23acf28b22da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6150685969405552"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_valid_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "414493e6-01b2-42b2-b6d6-077d7ccf380a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Fused window process have not been installed. Please refer to get_started.md for installation.\n"
     ]
    }
   ],
   "source": [
    "from models.swin_transformer import SwinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cd47b73-bf62-4a65-b506-486826fabe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_model = SwinTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e598c46-b421-4eb4-ad51-da554932ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = swin_model.head.in_features\n",
    "out_dim = 1\n",
    "swin_model.head = nn.Sequential(\n",
    "    #nn.Linear(hidden_dim, hidden_dim//16),\n",
    "    #nn.GELU(),\n",
    "    #nn.Linear(hidden_dim//16, out_dim),\n",
    "    # nn.Linear(hidden_dim, out_dim),\n",
    "    nn.Linear(hidden_dim, out_dim),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2989591c-fb77-41cb-9deb-752cd39a4d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      dim=192, input_resolution=(56, 56), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(56, 56), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(7, 7), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=192, input_resolution=(56, 56), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=192, window_size=(7, 7), num_heads=6\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.009)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(56, 56), dim=192\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      dim=384, input_resolution=(28, 28), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(28, 28), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.017)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=384, input_resolution=(28, 28), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=384, window_size=(7, 7), num_heads=12\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.026)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(28, 28), dim=384\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      dim=768, input_resolution=(14, 14), depth=18\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.035)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.043)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.052)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.061)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.070)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.078)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.087)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.096)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.104)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.113)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.122)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.130)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.139)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.148)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.157)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.165)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.174)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): SwinTransformerBlock(\n",
       "          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=768, window_size=(7, 7), num_heads=24\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.183)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMerging(\n",
       "        input_resolution=(14, 14), dim=768\n",
       "        (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "        (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      dim=1536, input_resolution=(7, 7), depth=2\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerBlock(\n",
       "          dim=1536, input_resolution=(7, 7), num_heads=48, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=1536, window_size=(7, 7), num_heads=48\n",
       "            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.191)\n",
       "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          dim=1536, input_resolution=(7, 7), num_heads=48, window_size=7, shift_size=0, mlp_ratio=4.0\n",
       "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            dim=1536, window_size=(7, 7), num_heads=48\n",
       "            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.200)\n",
       "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_model.load_state_dict(torch.load('./checkpoints/model_swin.ckpt',map_location={'cuda:1': 'cuda:0'}), strict=True)\n",
    "swin_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ddcc518-d7ae-42a4-a903-849907ce6cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6149019208118767\n"
     ]
    }
   ],
   "source": [
    "y_pred_02, y_true_02, metric_valid_02 = val_epoch(swin_model, test_loader)\n",
    "\n",
    "print(metric_valid_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d98079f5-baec-4c52-bd78-383abdacffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = (y_pred_01, y_true_01, y_pred_02, y_true_02)\n",
    "pd.to_pickle(output, './checkpoints/outputs.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
