{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fbe7cea-b614-43a7-a8cc-24b09ed69882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# from openslide import OpenSlide\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import (\n",
    "    ConcatDataset,\n",
    "    DataLoader,\n",
    "    Dataset,\n",
    "    Subset,\n",
    "    SubsetRandomSampler,\n",
    "    TensorDataset,\n",
    "    random_split,\n",
    ")\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# import einops\n",
    "\n",
    "# from eval_metrics import print_metrics_regression\n",
    "from sklearn import metrics as sklearn_metrics\n",
    "from models.slice_att import SliceAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64e71cd1-5d16-472f-80be-28d5fba31e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_pickle(f'./datasets_mini/train_x.pkl')\n",
    "train_y = pd.read_pickle(f'./datasets_mini/train_y.pkl')\n",
    "train_id = pd.read_pickle(f'./datasets_mini/train_id.pkl')\n",
    "\n",
    "test_x = pd.read_pickle(f'./datasets_mini/test_x.pkl')\n",
    "test_y = pd.read_pickle(f'./datasets_mini/test_y.pkl')\n",
    "test_id = pd.read_pickle(f'./datasets_mini/test_id.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd251bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 4.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_label = train_y.min().item()\n",
    "max_label = train_y.max().item()\n",
    "train_y = (train_y-min_label)/(max_label-min_label)\n",
    "test_y = (test_y-min_label)/(max_label-min_label)\n",
    "\n",
    "min_label, max_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81058b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_norm(x, min_label=min_label, max_label=max_label):\n",
    "    return (x-min_label)/(max_label-min_label)\n",
    "\n",
    "def reverse_min_max_norm(x, min_label=min_label, max_label=max_label):\n",
    "    return x*(max_label-min_label)+min_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "489c506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse normalization from Imagenet -> breast cancer dataset\n",
    "\n",
    "transform_dataset = transforms.Compose([\n",
    "        transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255], std=[1/0.229, 1/0.224, 1/0.255]),\n",
    "        transforms.Normalize([0.92740107, 0.90446373, 0.94529596], [0.02340832, 0.06800389, 0.04525188]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcf8426b-56f3-4413-91ff-632597396188",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, x, y, biopsy_id):\n",
    "        self.x = x # img_tensor_list\n",
    "        self.y = y # label\n",
    "        self.biopsy_id = np.array(biopsy_id)\n",
    "        self.id_set = np.unique(self.biopsy_id)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.id_set)    \n",
    "    \n",
    "    def __getitem__(self, index, transform=False):\n",
    "        cur_id = self.id_set[index]\n",
    "        cur_x = self.x[self.biopsy_id == cur_id]\n",
    "        cur_y = self.y[self.biopsy_id == cur_id][0]\n",
    "        cur_len = len(cur_x)\n",
    "        \n",
    "        if transform:\n",
    "            x_tensor = transform_dataset(cur_x)\n",
    "            return cur_x, cur_y, cur_len\n",
    "        return cur_x, cur_y, cur_len\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    imgs, labels, lens = zip(*batch)\n",
    "    imgs = torch.cat(imgs, dim=0)\n",
    "    labels = torch.stack(labels)\n",
    "    lens = torch.tensor(list(lens), dtype=torch.int32)\n",
    "    lens = torch.cumsum(lens, dim=0)\n",
    "    return imgs, labels, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7aec716-267c-45e4-a93d-ec074edd8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "epochs = 50\n",
    "learning_rate = 2e-4\n",
    "momentum = 0.9\n",
    "weight_decay = 0 # 1e-8\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d59a4783-0f99-4928-a666-f5a723c37bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_x, train_y, train_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_dataset = ImageDataset(test_x, test_y, test_id)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1051102c-ceb1-4271-8444-7eaeba3bdd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    return loss_fn(y_pred, y_true)\n",
    "\n",
    "def focal_mse_loss(inputs, targets, activate='sigmoid', beta=.2, gamma=1):\n",
    "    loss = (inputs - targets) ** 2\n",
    "    loss *= (torch.tanh(beta * torch.abs(inputs - targets))) ** gamma if activate == 'tanh' else \\\n",
    "        (2 * torch.sigmoid(beta * torch.abs(inputs - targets)) - 1) ** gamma\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def huber_loss(inputs, targets, beta=1.):\n",
    "    l1_loss = torch.abs(inputs - targets)\n",
    "    cond = l1_loss < beta\n",
    "    loss = torch.where(cond, 0.5 * l1_loss ** 2 / beta, l1_loss - 0.5 * beta)\n",
    "    loss = torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "criterion = mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe0b0f76-e41e-440b-93e8-9a605e5799b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, model_att, dataloader, loss_fn, optimizer, scheduler):\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "    model_att.train()\n",
    "    for step, data in enumerate(dataloader):\n",
    "        batch_x, batch_y, batch_len = data\n",
    "        batch_x, batch_y = (\n",
    "            batch_x.float().to(device),\n",
    "            batch_y.float().to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        # print(batch_x.device, batch_x.shape)\n",
    "        # print(next(model.parameters()).is_cuda)\n",
    "        feature_vec = model(batch_x)\n",
    "        output = model_att(feature_vec, batch_len)\n",
    "        output = torch.squeeze(output, dim=1)\n",
    "        \n",
    "        loss = loss_fn(output, batch_y)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    metric_train_loss = np.array(train_loss).mean()\n",
    "    scheduler.step(metric_train_loss)\n",
    "    return metric_train_loss\n",
    "\n",
    "def val_epoch(model, model_att, dataloader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    model.eval()\n",
    "    model_att.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(dataloader):\n",
    "            # print(step)\n",
    "            batch_x, batch_y, batch_len = data\n",
    "            batch_x, batch_y = (\n",
    "                batch_x.float().to(device),\n",
    "                batch_y.float().to(device),\n",
    "            )\n",
    "            feature_vec = model(batch_x)\n",
    "            output = model_att(feature_vec, batch_len)\n",
    "            output = torch.squeeze(output, dim=1)\n",
    "            output = output.detach().cpu().numpy().tolist()\n",
    "            batch_y = batch_y.detach().cpu().numpy().tolist()\n",
    "            y_pred.extend(output)\n",
    "            y_true.extend(batch_y)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = reverse_min_max_norm(y_pred)\n",
    "    y_true = reverse_min_max_norm(y_true)\n",
    "\n",
    "    mse = sklearn_metrics.mean_squared_error(y_true, y_pred)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5842d16-104b-4420-9b8b-2ba22a4772ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.resnet18(num_classes=1)\n",
    "model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "hidden_dim = model.fc.in_features\n",
    "model.fc = nn.Sequential()\n",
    "model_att = SliceAttention(hidden_dim, hidden_dim//8)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(model_att.parameters()), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "# model.load_state_dict(torch.load('./checkpoints/resnet18-f37072fd.pth'), strict=False)\n",
    "# model.load_state_dict(torch.load('./checkpoints/resnet50-11ad3fa6.pth'), strict=False)\n",
    "\n",
    "model.to(device)\n",
    "model_att.to(device)\n",
    "# set_parameter_requires_grad(model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecda0015-3854-45a3-bdeb-46ccc2a963ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.08266448974609375\n",
      "Val Score: 0.49302292362571143\n",
      "Saving best model ...\n",
      "Epoch 1: Loss = 0.04256656765937805\n",
      "Val Score: 0.37895327921013583\n",
      "Saving best model ...\n",
      "Epoch 2: Loss = 0.0419778898358345\n",
      "Val Score: 0.5346572665292936\n",
      "Epoch 3: Loss = 0.042613279074430466\n",
      "Val Score: 0.5108245264618505\n",
      "Epoch 4: Loss = 0.03430700674653053\n",
      "Val Score: 0.3903572415563362\n",
      "Epoch 5: Loss = 0.02525983192026615\n",
      "Val Score: 0.31059334295862334\n",
      "Saving best model ...\n",
      "Epoch 6: Loss = 0.020161766558885574\n",
      "Val Score: 0.30864520851371646\n",
      "Saving best model ...\n",
      "Epoch 7: Loss = 0.018717989325523376\n",
      "Val Score: 0.3358035080638183\n",
      "Epoch 8: Loss = 0.018421906977891922\n",
      "Val Score: 0.3361505342605279\n",
      "Epoch 9: Loss = 0.017290957272052765\n",
      "Val Score: 0.31166993208170124\n",
      "Epoch 10: Loss = 0.015002255327999592\n",
      "Val Score: 0.2859653541918664\n",
      "Saving best model ...\n",
      "Epoch 11: Loss = 0.012182102538645267\n",
      "Val Score: 0.2664742755606964\n",
      "Saving best model ...\n",
      "Epoch 12: Loss = 0.009751765988767147\n",
      "Val Score: 0.2758926218238697\n",
      "Epoch 13: Loss = 0.008359508588910103\n",
      "Val Score: 0.3043289277511853\n",
      "Epoch 14: Loss = 0.007950889877974987\n",
      "Val Score: 0.3349963470348071\n",
      "Epoch 15: Loss = 0.007866190746426582\n",
      "Val Score: 0.3424960882375263\n",
      "Epoch 16: Loss = 0.007380051072686911\n",
      "Val Score: 0.34266172758010394\n",
      "Epoch 17: Loss = 0.006281574256718159\n",
      "Val Score: 0.32842244600163467\n",
      "Epoch 18: Loss = 0.004959185607731342\n",
      "Val Score: 0.3078339359541976\n",
      "Epoch 19: Loss = 0.003949680365622044\n",
      "Val Score: 0.2978538269790567\n",
      "Epoch 20: Loss = 0.0034706536680459976\n",
      "Val Score: 0.2984138760748877\n",
      "Epoch 21: Loss = 0.0033592756371945143\n",
      "Val Score: 0.290116618501888\n",
      "Epoch 22: Loss = 0.003238038392737508\n",
      "Val Score: 0.2750903368914786\n",
      "Epoch 23: Loss = 0.0028541358187794685\n",
      "Val Score: 0.26016969132867035\n",
      "Saving best model ...\n",
      "Epoch 24: Loss = 0.002253908198326826\n",
      "Val Score: 0.24920282688825884\n",
      "Saving best model ...\n",
      "Epoch 25: Loss = 0.0016749849310144782\n",
      "Val Score: 0.24541135498447506\n",
      "Saving best model ...\n",
      "Epoch 26: Loss = 0.0013129438739269972\n",
      "Val Score: 0.24231632078856774\n",
      "Saving best model ...\n",
      "Epoch 27: Loss = 0.0011943255085498095\n",
      "Val Score: 0.24012797802234456\n",
      "Saving best model ...\n",
      "Epoch 28: Loss = 0.0011757665779441595\n",
      "Val Score: 0.23808096031499612\n",
      "Saving best model ...\n",
      "Epoch 29: Loss = 0.0010809829691424966\n",
      "Val Score: 0.23583817963908746\n",
      "Saving best model ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m best_score \u001b[39m=\u001b[39m \u001b[39m1e8\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[39m# print(f'Running epoch {epoch} ...')\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     train_loss \u001b[39m=\u001b[39m train_epoch(\n\u001b[1;32m      5\u001b[0m         model,\n\u001b[1;32m      6\u001b[0m         model_att,\n\u001b[1;32m      7\u001b[0m         train_loader,\n\u001b[1;32m      8\u001b[0m         criterion,\n\u001b[1;32m      9\u001b[0m         optimizer,\n\u001b[1;32m     10\u001b[0m         scheduler\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m: Loss = \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn [23], line 14\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, model_att, dataloader, loss_fn, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m \u001b[39m# print(batch_x.device, batch_x.shape)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# print(next(model.parameters()).is_cuda)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m feature_vec \u001b[39m=\u001b[39m model(batch_x)\n\u001b[1;32m     15\u001b[0m output \u001b[39m=\u001b[39m model_att(feature_vec, batch_len)\n\u001b[1;32m     16\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(output, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torchvision/models/resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torchvision/models/resnet.py:146\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    144\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m--> 146\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    147\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_score = 1e8\n",
    "for epoch in range(epochs):\n",
    "    # print(f'Running epoch {epoch} ...')\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        model_att,\n",
    "        train_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler\n",
    "    )\n",
    "    print(f\"Epoch {epoch}: Loss = {train_loss}\")\n",
    "    if epoch % 1 == 0:\n",
    "        metric_valid = val_epoch(model, model_att, test_loader)\n",
    "        print(\"Val Score:\", metric_valid)\n",
    "        if metric_valid < best_score:\n",
    "            best_score = metric_valid\n",
    "            print(\"Saving best model ...\")\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                f\"./checkpoints/model_resnet50_1028.ckpt\",\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c5ece5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21758774127243202"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score  #sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd2bda1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19854353574712819"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score  #relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba63394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a13a82820b1c6573eba0aaa34eb88150924a449653129551d604995f65c01d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
